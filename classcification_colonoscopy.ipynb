{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e8873b-15ba-4313-9446-ee006440f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchmetrics.functional import accuracy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as transform\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torchmetrics.functional import accuracy\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import DiceLoss,DiceCELoss ,DiceFocalLoss ,MaskedDiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "import random\n",
    "import cv2\n",
    "from torchmetrics import Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfebdd47-c1a3-42d2-98a5-61ac22d415e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda', 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device , num_gpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba9cc0f-04fa-4654-b19a-a7c5adeac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary(org_img):\n",
    "    #print(np.any(org_img))\n",
    "    if np.any(org_img):\n",
    "        normalized_img = (org_img - np.min(org_img)) / (np.max(org_img) - np.min(org_img))\n",
    "        normalized_img[normalized_img < 0.5] = 0\n",
    "        normalized_img[normalized_img >= 0.5] = 1\n",
    "        return normalized_img\n",
    "    else:\n",
    "        return org_img\n",
    "def rgb(org_img):\n",
    "    normalized_img = (org_img - np.min(org_img)) / (np.max(org_img) - np.min(org_img))\n",
    "    return normalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f900dbd4-c962-40cc-ae33-568d92323872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_dataset(Dataset):\n",
    "    def __init__(self,image_filenames,mask_filenames,transforms=None):\n",
    "        self.image_dir = \"/scratch/akaniyar/colonoscopy/images/\"\n",
    "        self.mask_dir = \"/scratch/akaniyar/colonoscopy/masks/\"\n",
    "        self.transform = transforms\n",
    "        #self.mask_transform = transforms.Compose([\n",
    "        self.mask_transform = transform.Compose([transform.ToTensor()])\n",
    "    \n",
    "        self.image_filenames = image_filenames\n",
    "        self.mask_filenames = mask_filenames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = tifffile.imread(os.path.join(self.image_dir, self.image_filenames[idx]))\n",
    "        label = tifffile.imread(os.path.join(self.mask_dir, self.mask_filenames[idx]))\n",
    "        # print(np.unique(label))\n",
    "        if len(np.unique(label))>1:\n",
    "            label = 1\n",
    "        else :\n",
    "            label = 0\n",
    "        #print(np.unique(img),np.unique(label))\n",
    "        #label = binary(label.astype(np.float32))\n",
    "        #img = rgb(img)\n",
    "        if self.transform is not None:\n",
    "            img, label = self.transform(img), torch.tensor(label).unsqueeze(0)\n",
    "            \n",
    "        label = label.to(torch.float32)\n",
    "        return img, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d711daf-014c-4dc5-b718-0674c80eb59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths\n",
    "file1_path = '/home/akaniyar/Colonoscopy/data_load_text/np_filenames.txt'\n",
    "file2_path = '/home/akaniyar/Colonoscopy/data_load_text/np_maskfilenames.txt'\n",
    "file3_path = '/home/akaniyar/Colonoscopy/data_load_text/wp_filenames.txt'\n",
    "file4_path = '/home/akaniyar/Colonoscopy/data_load_text/wp_maskfilenames.txt'\n",
    "\n",
    "# Create empty lists to store image paths\n",
    "np_filenames = []\n",
    "np_maskfilenames = []\n",
    "wp_filenames = []\n",
    "wp_maskfilenames = []\n",
    "\n",
    "#Read image paths from file1.txt and append to np_filenames list\n",
    "with open(file1_path, 'r') as file1:\n",
    "    for line in file1:\n",
    "        np_filenames.append(line.strip())\n",
    "\n",
    "# Read image paths from file2.txt and append to np_maskfilenames list\n",
    "with open(file2_path, 'r') as file2:\n",
    "    for line in file2:\n",
    "        np_maskfilenames.append(line.strip())\n",
    "\n",
    "# Read image paths from file3.txt and append to wp_filenames list\n",
    "with open(file3_path, 'r') as file3:\n",
    "    for line in file3:\n",
    "        wp_filenames.append(line.strip())\n",
    "\n",
    "# Read image paths from file4.txt and append to wp_maskfilenames list\n",
    "with open(file4_path, 'r') as file4:\n",
    "    for line in file4:\n",
    "        wp_maskfilenames.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7605d0e4-64d5-4c9d-a73b-f1c87c9d12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15046 15046\n",
      "3856 3856\n"
     ]
    }
   ],
   "source": [
    "print(len(np_filenames),len(np_maskfilenames))\n",
    "print(len(wp_filenames),len(wp_maskfilenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041b049f-4064-4b44-a677-0252a4ac79cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18902 18902\n"
     ]
    }
   ],
   "source": [
    "wp_train_size = int(1.0*len(wp_filenames))\n",
    "np_train_size = int(1.0*len(np_filenames))\n",
    "\n",
    "\n",
    "image_filenames = wp_filenames[:wp_train_size]+np_filenames[:np_train_size] \n",
    "mask_filenames = wp_maskfilenames[:wp_train_size] +  np_maskfilenames[:np_train_size] \n",
    "print(len(image_filenames),len(mask_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32bea49-9934-4059-9c6e-cea6422dd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transform.Compose([\n",
    "            transform.ToPILImage(),\n",
    "            transform.Resize((128,128),InterpolationMode.BICUBIC),\n",
    "            transform.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51929e45-703c-425a-a999-b587db97d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "image_filenames_train, image_filenames_test, mask_filenames_train, mask_filenames_test = train_test_split(\n",
    "    image_filenames, mask_filenames, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further splitting the train set into train and validation sets\n",
    "image_filenames_train, image_filenames_val, mask_filenames_train, mask_filenames_val = train_test_split(\n",
    "    image_filenames_train, mask_filenames_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Creating your segmentation dataset using the split filenames\n",
    "train_dataset = classification_dataset(image_filenames=image_filenames_train,\n",
    "                                     mask_filenames=mask_filenames_train,\n",
    "                                     transforms=data_transform)\n",
    "\n",
    "val_dataset = classification_dataset(image_filenames=image_filenames_val,\n",
    "                                   mask_filenames=mask_filenames_val,\n",
    "                                   transforms=data_transform)\n",
    "\n",
    "test_dataset = classification_dataset(image_filenames=image_filenames_test,\n",
    "                                    mask_filenames=mask_filenames_test,\n",
    "                                    transforms=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8565b662-5fc5-4cf4-9643-4b62769ff308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13608, 1513, 3781)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(val_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee2c320a-0a20-4870-b0c4-b358e34482dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True , num_workers = num_gpus)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32,shuffle=True,num_workers = num_gpus)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True,num_workers = num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1389ac0-4d2a-4874-be78-04a0c59dfd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128]) torch.Size([1])\n",
      "tensor([0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275, 0.0314,\n",
      "        0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588, 0.0627, 0.0667,\n",
      "        0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902, 0.0941, 0.0980, 0.1020,\n",
      "        0.1059, 0.1098, 0.1137, 0.1176, 0.1216, 0.1255, 0.1294, 0.1333, 0.1373,\n",
      "        0.1412, 0.1451, 0.1490, 0.1529, 0.1569, 0.1608, 0.1647, 0.1686, 0.1725,\n",
      "        0.1765, 0.1804, 0.1843, 0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078,\n",
      "        0.2118, 0.2157, 0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431,\n",
      "        0.2471, 0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
      "        0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098, 0.3137,\n",
      "        0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412, 0.3451, 0.3490,\n",
      "        0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725, 0.3765, 0.3804, 0.3843,\n",
      "        0.3882, 0.3922, 0.3961, 0.4000, 0.4039, 0.4078, 0.4118, 0.4157, 0.4196,\n",
      "        0.4235, 0.4275, 0.4314, 0.4353, 0.4392, 0.4431, 0.4471, 0.4510, 0.4549,\n",
      "        0.4588, 0.4627, 0.4667, 0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902,\n",
      "        0.4941, 0.4980, 0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255,\n",
      "        0.5294, 0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608,\n",
      "        0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922, 0.5961,\n",
      "        0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235, 0.6275, 0.6314,\n",
      "        0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549, 0.6588, 0.6627, 0.6667,\n",
      "        0.6706, 0.6745, 0.6784, 0.6824, 0.6863, 0.6902, 0.6941, 0.6980, 0.7020,\n",
      "        0.7059, 0.7098, 0.7137, 0.7176, 0.7216, 0.7255, 0.7294, 0.7333, 0.7373,\n",
      "        0.7412, 0.7451, 0.7490, 0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725,\n",
      "        0.7765, 0.7804, 0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078,\n",
      "        0.8118, 0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
      "        0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745, 0.8784,\n",
      "        0.8824, 0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059, 0.9098, 0.9137,\n",
      "        0.9176, 0.9216, 0.9255, 0.9294, 0.9333, 0.9373, 0.9412, 0.9451, 0.9490,\n",
      "        0.9529, 0.9569, 0.9608, 0.9647, 0.9686, 0.9725, 0.9765, 0.9804, 0.9843,\n",
      "        0.9882, 0.9922, 0.9961, 1.0000])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "i,l = train_dataset[10000]\n",
    "print(i.shape ,l.shape)\n",
    "print(torch.unique(i) )\n",
    "print(i.dtype,l.dtype)\n",
    "# l = (l == 1.0000 )\n",
    "# l2 = (l==1)\n",
    "# plt.imshow(i.permute(1,2,0))\n",
    "# plt.imshow(l.permute(1,2,0),alpha = 0.4,cmap = \"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335a36df-fbd3-4284-8687-5492fa3bd104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Classification_head\n",
    "from torch.nn.parallel import DataParallel\n",
    "# from torchvision.models import swin_b as swin_tiny\n",
    "from torchvision.models import convnext_base as convb\n",
    "# from resunet_pretrain import Resenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e3eda08-7dc0-4ea2-a32a-a27bf816fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a34cf5d-8fc8-4b48-8afd-1dc0ed2802a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "import torch\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "    nn.ReLU(inplace=True),\n",
    "  )\n",
    "\n",
    "class Resenc(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = torchvision.models.resnet18(weights= \"DEFAULT\")\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        \n",
    "\n",
    "\n",
    "        # self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
    "        # self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, n_class)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # x_original = self.conv_original_size0(input)\n",
    "        # x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        class_out = self.average_pool(layer4)\n",
    "        class_out = class_out.view(class_out.size(0), -1)\n",
    "        class_out = self.fc(class_out)\n",
    "        return class_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9754be7c-1a2a-4b85-922a-646d2e027132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Classification_head(n_channels=3,n_classes=1)\n",
    "model = convb(weights='DEFAULT')\n",
    "# model.head = nn.Sequential(nn.Linear(in_features=1024, out_features=512, bias=True),\n",
    "#                            nn.Linear(in_features=512, out_features=256, bias=True),\n",
    "#                            nn.Linear(in_features=256, out_features=64, bias=True),\n",
    "#                            nn.Linear(in_features=64, out_features=32, bias=True),\n",
    "#                            nn.Linear(in_features=32, out_features=1, bias=True))\n",
    "# model = Resenc(1)\n",
    "model.classifier[2] = nn.Sequential(nn.Linear(in_features=1024, out_features=1, bias=True))\n",
    "model = DataParallel(model, device_ids=list(range(num_gpus)), dim=0)\n",
    "model = model.to(\"cuda\")\n",
    "# summary(model,(3,256,256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e588f-e8f8-45ee-a66f-7429f4c3541f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa5456ba-b718-4a53-8d8f-be5a817bdbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 64, 64]           6,272\n",
      "            Conv2d-2          [-1, 128, 64, 64]           6,272\n",
      "       LayerNorm2d-3          [-1, 128, 64, 64]             256\n",
      "       LayerNorm2d-4          [-1, 128, 64, 64]             256\n",
      "            Conv2d-5          [-1, 128, 64, 64]           6,400\n",
      "           Permute-6          [-1, 64, 64, 128]               0\n",
      "            Conv2d-7          [-1, 128, 64, 64]           6,400\n",
      "         LayerNorm-8          [-1, 64, 64, 128]             256\n",
      "           Permute-9          [-1, 64, 64, 128]               0\n",
      "        LayerNorm-10          [-1, 64, 64, 128]             256\n",
      "           Linear-11          [-1, 64, 64, 512]          66,048\n",
      "           Linear-12          [-1, 64, 64, 512]          66,048\n",
      "             GELU-13          [-1, 64, 64, 512]               0\n",
      "             GELU-14          [-1, 64, 64, 512]               0\n",
      "           Linear-15          [-1, 64, 64, 128]          65,664\n",
      "           Linear-16          [-1, 64, 64, 128]          65,664\n",
      "          Permute-17          [-1, 128, 64, 64]               0\n",
      "          Permute-18          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-19          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-20          [-1, 128, 64, 64]               0\n",
      "          CNBlock-21          [-1, 128, 64, 64]               0\n",
      "          CNBlock-22          [-1, 128, 64, 64]               0\n",
      "           Conv2d-23          [-1, 128, 64, 64]           6,400\n",
      "           Conv2d-24          [-1, 128, 64, 64]           6,400\n",
      "          Permute-25          [-1, 64, 64, 128]               0\n",
      "        LayerNorm-26          [-1, 64, 64, 128]             256\n",
      "          Permute-27          [-1, 64, 64, 128]               0\n",
      "        LayerNorm-28          [-1, 64, 64, 128]             256\n",
      "           Linear-29          [-1, 64, 64, 512]          66,048\n",
      "           Linear-30          [-1, 64, 64, 512]          66,048\n",
      "             GELU-31          [-1, 64, 64, 512]               0\n",
      "             GELU-32          [-1, 64, 64, 512]               0\n",
      "           Linear-33          [-1, 64, 64, 128]          65,664\n",
      "           Linear-34          [-1, 64, 64, 128]          65,664\n",
      "          Permute-35          [-1, 128, 64, 64]               0\n",
      "          Permute-36          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-37          [-1, 128, 64, 64]               0\n",
      "          CNBlock-38          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-39          [-1, 128, 64, 64]               0\n",
      "          CNBlock-40          [-1, 128, 64, 64]               0\n",
      "           Conv2d-41          [-1, 128, 64, 64]           6,400\n",
      "           Conv2d-42          [-1, 128, 64, 64]           6,400\n",
      "          Permute-43          [-1, 64, 64, 128]               0\n",
      "        LayerNorm-44          [-1, 64, 64, 128]             256\n",
      "          Permute-45          [-1, 64, 64, 128]               0\n",
      "        LayerNorm-46          [-1, 64, 64, 128]             256\n",
      "           Linear-47          [-1, 64, 64, 512]          66,048\n",
      "           Linear-48          [-1, 64, 64, 512]          66,048\n",
      "             GELU-49          [-1, 64, 64, 512]               0\n",
      "             GELU-50          [-1, 64, 64, 512]               0\n",
      "           Linear-51          [-1, 64, 64, 128]          65,664\n",
      "           Linear-52          [-1, 64, 64, 128]          65,664\n",
      "          Permute-53          [-1, 128, 64, 64]               0\n",
      "          Permute-54          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-55          [-1, 128, 64, 64]               0\n",
      "          CNBlock-56          [-1, 128, 64, 64]               0\n",
      "  StochasticDepth-57          [-1, 128, 64, 64]               0\n",
      "      LayerNorm2d-58          [-1, 128, 64, 64]             256\n",
      "          CNBlock-59          [-1, 128, 64, 64]               0\n",
      "      LayerNorm2d-60          [-1, 128, 64, 64]             256\n",
      "           Conv2d-61          [-1, 256, 32, 32]         131,328\n",
      "           Conv2d-62          [-1, 256, 32, 32]         131,328\n",
      "           Conv2d-63          [-1, 256, 32, 32]          12,800\n",
      "           Conv2d-64          [-1, 256, 32, 32]          12,800\n",
      "          Permute-65          [-1, 32, 32, 256]               0\n",
      "          Permute-66          [-1, 32, 32, 256]               0\n",
      "        LayerNorm-67          [-1, 32, 32, 256]             512\n",
      "        LayerNorm-68          [-1, 32, 32, 256]             512\n",
      "           Linear-69         [-1, 32, 32, 1024]         263,168\n",
      "           Linear-70         [-1, 32, 32, 1024]         263,168\n",
      "             GELU-71         [-1, 32, 32, 1024]               0\n",
      "             GELU-72         [-1, 32, 32, 1024]               0\n",
      "           Linear-73          [-1, 32, 32, 256]         262,400\n",
      "           Linear-74          [-1, 32, 32, 256]         262,400\n",
      "          Permute-75          [-1, 256, 32, 32]               0\n",
      "          Permute-76          [-1, 256, 32, 32]               0\n",
      "  StochasticDepth-77          [-1, 256, 32, 32]               0\n",
      "          CNBlock-78          [-1, 256, 32, 32]               0\n",
      "  StochasticDepth-79          [-1, 256, 32, 32]               0\n",
      "          CNBlock-80          [-1, 256, 32, 32]               0\n",
      "           Conv2d-81          [-1, 256, 32, 32]          12,800\n",
      "           Conv2d-82          [-1, 256, 32, 32]          12,800\n",
      "          Permute-83          [-1, 32, 32, 256]               0\n",
      "          Permute-84          [-1, 32, 32, 256]               0\n",
      "        LayerNorm-85          [-1, 32, 32, 256]             512\n",
      "        LayerNorm-86          [-1, 32, 32, 256]             512\n",
      "           Linear-87         [-1, 32, 32, 1024]         263,168\n",
      "           Linear-88         [-1, 32, 32, 1024]         263,168\n",
      "             GELU-89         [-1, 32, 32, 1024]               0\n",
      "             GELU-90         [-1, 32, 32, 1024]               0\n",
      "           Linear-91          [-1, 32, 32, 256]         262,400\n",
      "           Linear-92          [-1, 32, 32, 256]         262,400\n",
      "          Permute-93          [-1, 256, 32, 32]               0\n",
      "          Permute-94          [-1, 256, 32, 32]               0\n",
      "  StochasticDepth-95          [-1, 256, 32, 32]               0\n",
      "          CNBlock-96          [-1, 256, 32, 32]               0\n",
      "  StochasticDepth-97          [-1, 256, 32, 32]               0\n",
      "          CNBlock-98          [-1, 256, 32, 32]               0\n",
      "           Conv2d-99          [-1, 256, 32, 32]          12,800\n",
      "          Conv2d-100          [-1, 256, 32, 32]          12,800\n",
      "         Permute-101          [-1, 32, 32, 256]               0\n",
      "       LayerNorm-102          [-1, 32, 32, 256]             512\n",
      "         Permute-103          [-1, 32, 32, 256]               0\n",
      "       LayerNorm-104          [-1, 32, 32, 256]             512\n",
      "          Linear-105         [-1, 32, 32, 1024]         263,168\n",
      "          Linear-106         [-1, 32, 32, 1024]         263,168\n",
      "            GELU-107         [-1, 32, 32, 1024]               0\n",
      "            GELU-108         [-1, 32, 32, 1024]               0\n",
      "          Linear-109          [-1, 32, 32, 256]         262,400\n",
      "         Permute-110          [-1, 256, 32, 32]               0\n",
      "          Linear-111          [-1, 32, 32, 256]         262,400\n",
      "         Permute-112          [-1, 256, 32, 32]               0\n",
      " StochasticDepth-113          [-1, 256, 32, 32]               0\n",
      "         CNBlock-114          [-1, 256, 32, 32]               0\n",
      " StochasticDepth-115          [-1, 256, 32, 32]               0\n",
      "     LayerNorm2d-116          [-1, 256, 32, 32]             512\n",
      "         CNBlock-117          [-1, 256, 32, 32]               0\n",
      "     LayerNorm2d-118          [-1, 256, 32, 32]             512\n",
      "          Conv2d-119          [-1, 512, 16, 16]         524,800\n",
      "          Conv2d-120          [-1, 512, 16, 16]         524,800\n",
      "          Conv2d-121          [-1, 512, 16, 16]          25,600\n",
      "         Permute-122          [-1, 16, 16, 512]               0\n",
      "          Conv2d-123          [-1, 512, 16, 16]          25,600\n",
      "       LayerNorm-124          [-1, 16, 16, 512]           1,024\n",
      "         Permute-125          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-126          [-1, 16, 16, 512]           1,024\n",
      "          Linear-127         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-128         [-1, 16, 16, 2048]               0\n",
      "          Linear-129         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-130         [-1, 16, 16, 2048]               0\n",
      "          Linear-131          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-132          [-1, 512, 16, 16]               0\n",
      "          Linear-133          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-134          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-135          [-1, 512, 16, 16]               0\n",
      "         CNBlock-136          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-137          [-1, 512, 16, 16]               0\n",
      "          Conv2d-138          [-1, 512, 16, 16]          25,600\n",
      "         CNBlock-139          [-1, 512, 16, 16]               0\n",
      "         Permute-140          [-1, 16, 16, 512]               0\n",
      "          Conv2d-141          [-1, 512, 16, 16]          25,600\n",
      "       LayerNorm-142          [-1, 16, 16, 512]           1,024\n",
      "         Permute-143          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-144          [-1, 16, 16, 512]           1,024\n",
      "          Linear-145         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-146         [-1, 16, 16, 2048]               0\n",
      "          Linear-147         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-148          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-149          [-1, 512, 16, 16]               0\n",
      "            GELU-150         [-1, 16, 16, 2048]               0\n",
      "          Linear-151          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-152          [-1, 512, 16, 16]               0\n",
      "         CNBlock-153          [-1, 512, 16, 16]               0\n",
      "         Permute-154          [-1, 512, 16, 16]               0\n",
      "          Conv2d-155          [-1, 512, 16, 16]          25,600\n",
      "         Permute-156          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-157          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-158          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-159          [-1, 512, 16, 16]               0\n",
      "          Conv2d-160          [-1, 512, 16, 16]          25,600\n",
      "          Linear-161         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-162          [-1, 16, 16, 512]               0\n",
      "            GELU-163         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-164          [-1, 16, 16, 512]           1,024\n",
      "          Linear-165          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-166         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-167          [-1, 512, 16, 16]               0\n",
      "            GELU-168         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-169          [-1, 512, 16, 16]               0\n",
      "         CNBlock-170          [-1, 512, 16, 16]               0\n",
      "          Linear-171          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-172          [-1, 512, 16, 16]          25,600\n",
      "         Permute-173          [-1, 512, 16, 16]               0\n",
      "         Permute-174          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-175          [-1, 16, 16, 512]           1,024\n",
      " StochasticDepth-176          [-1, 512, 16, 16]               0\n",
      "         CNBlock-177          [-1, 512, 16, 16]               0\n",
      "          Linear-178         [-1, 16, 16, 2048]       1,050,624\n",
      "          Conv2d-179          [-1, 512, 16, 16]          25,600\n",
      "            GELU-180         [-1, 16, 16, 2048]               0\n",
      "         Permute-181          [-1, 16, 16, 512]               0\n",
      "          Linear-182          [-1, 16, 16, 512]       1,049,088\n",
      "       LayerNorm-183          [-1, 16, 16, 512]           1,024\n",
      "         Permute-184          [-1, 512, 16, 16]               0\n",
      "          Linear-185         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-186         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-187          [-1, 512, 16, 16]               0\n",
      "         CNBlock-188          [-1, 512, 16, 16]               0\n",
      "          Linear-189          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-190          [-1, 512, 16, 16]          25,600\n",
      "         Permute-191          [-1, 512, 16, 16]               0\n",
      "         Permute-192          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-193          [-1, 16, 16, 512]           1,024\n",
      " StochasticDepth-194          [-1, 512, 16, 16]               0\n",
      "         CNBlock-195          [-1, 512, 16, 16]               0\n",
      "          Conv2d-196          [-1, 512, 16, 16]          25,600\n",
      "          Linear-197         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-198          [-1, 16, 16, 512]               0\n",
      "            GELU-199         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-200          [-1, 16, 16, 512]           1,024\n",
      "          Linear-201          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-202          [-1, 512, 16, 16]               0\n",
      "          Linear-203         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-204         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-205          [-1, 512, 16, 16]               0\n",
      "         CNBlock-206          [-1, 512, 16, 16]               0\n",
      "          Linear-207          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-208          [-1, 512, 16, 16]          25,600\n",
      "         Permute-209          [-1, 512, 16, 16]               0\n",
      "         Permute-210          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-211          [-1, 16, 16, 512]           1,024\n",
      " StochasticDepth-212          [-1, 512, 16, 16]               0\n",
      "         CNBlock-213          [-1, 512, 16, 16]               0\n",
      "          Conv2d-214          [-1, 512, 16, 16]          25,600\n",
      "          Linear-215         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-216          [-1, 16, 16, 512]               0\n",
      "            GELU-217         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-218          [-1, 16, 16, 512]           1,024\n",
      "          Linear-219          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-220          [-1, 512, 16, 16]               0\n",
      "          Linear-221         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-222         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-223          [-1, 512, 16, 16]               0\n",
      "         CNBlock-224          [-1, 512, 16, 16]               0\n",
      "          Linear-225          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-226          [-1, 512, 16, 16]          25,600\n",
      "         Permute-227          [-1, 512, 16, 16]               0\n",
      "         Permute-228          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-229          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-230          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-231          [-1, 512, 16, 16]               0\n",
      "          Conv2d-232          [-1, 512, 16, 16]          25,600\n",
      "          Linear-233         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-234          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-235          [-1, 16, 16, 512]           1,024\n",
      "            GELU-236         [-1, 16, 16, 2048]               0\n",
      "          Linear-237          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-238         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-239          [-1, 512, 16, 16]               0\n",
      "            GELU-240         [-1, 16, 16, 2048]               0\n",
      "          Linear-241          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-242          [-1, 512, 16, 16]               0\n",
      "         CNBlock-243          [-1, 512, 16, 16]               0\n",
      "         Permute-244          [-1, 512, 16, 16]               0\n",
      "          Conv2d-245          [-1, 512, 16, 16]          25,600\n",
      "         Permute-246          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-247          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-248          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-249          [-1, 512, 16, 16]               0\n",
      "          Conv2d-250          [-1, 512, 16, 16]          25,600\n",
      "          Linear-251         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-252          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-253          [-1, 16, 16, 512]           1,024\n",
      "            GELU-254         [-1, 16, 16, 2048]               0\n",
      "          Linear-255          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-256         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-257          [-1, 512, 16, 16]               0\n",
      "            GELU-258         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-259          [-1, 512, 16, 16]               0\n",
      "          Linear-260          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-261          [-1, 512, 16, 16]               0\n",
      "         Permute-262          [-1, 512, 16, 16]               0\n",
      "          Conv2d-263          [-1, 512, 16, 16]          25,600\n",
      "         Permute-264          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-265          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-266          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-267          [-1, 512, 16, 16]               0\n",
      "          Conv2d-268          [-1, 512, 16, 16]          25,600\n",
      "          Linear-269         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-270          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-271          [-1, 16, 16, 512]           1,024\n",
      "            GELU-272         [-1, 16, 16, 2048]               0\n",
      "          Linear-273          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-274         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-275          [-1, 512, 16, 16]               0\n",
      "            GELU-276         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-277          [-1, 512, 16, 16]               0\n",
      "          Linear-278          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-279          [-1, 512, 16, 16]               0\n",
      "         Permute-280          [-1, 512, 16, 16]               0\n",
      "          Conv2d-281          [-1, 512, 16, 16]          25,600\n",
      "         Permute-282          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-283          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-284          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-285          [-1, 512, 16, 16]               0\n",
      "          Conv2d-286          [-1, 512, 16, 16]          25,600\n",
      "          Linear-287         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-288          [-1, 16, 16, 512]               0\n",
      "            GELU-289         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-290          [-1, 16, 16, 512]           1,024\n",
      "          Linear-291          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-292         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-293          [-1, 512, 16, 16]               0\n",
      "            GELU-294         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-295          [-1, 512, 16, 16]               0\n",
      "         CNBlock-296          [-1, 512, 16, 16]               0\n",
      "          Linear-297          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-298          [-1, 512, 16, 16]          25,600\n",
      "         Permute-299          [-1, 512, 16, 16]               0\n",
      "         Permute-300          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-301          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-302          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-303          [-1, 512, 16, 16]               0\n",
      "          Conv2d-304          [-1, 512, 16, 16]          25,600\n",
      "          Linear-305         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-306          [-1, 16, 16, 512]               0\n",
      "            GELU-307         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-308          [-1, 16, 16, 512]           1,024\n",
      "          Linear-309          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-310          [-1, 512, 16, 16]               0\n",
      "          Linear-311         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-312         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-313          [-1, 512, 16, 16]               0\n",
      "         CNBlock-314          [-1, 512, 16, 16]               0\n",
      "          Linear-315          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-316          [-1, 512, 16, 16]          25,600\n",
      "         Permute-317          [-1, 512, 16, 16]               0\n",
      "         Permute-318          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-319          [-1, 16, 16, 512]           1,024\n",
      " StochasticDepth-320          [-1, 512, 16, 16]               0\n",
      "         CNBlock-321          [-1, 512, 16, 16]               0\n",
      "          Conv2d-322          [-1, 512, 16, 16]          25,600\n",
      "          Linear-323         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-324         [-1, 16, 16, 2048]               0\n",
      "         Permute-325          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-326          [-1, 16, 16, 512]           1,024\n",
      "          Linear-327          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-328          [-1, 512, 16, 16]               0\n",
      "          Linear-329         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-330         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-331          [-1, 512, 16, 16]               0\n",
      "         CNBlock-332          [-1, 512, 16, 16]               0\n",
      "          Linear-333          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-334          [-1, 512, 16, 16]          25,600\n",
      "         Permute-335          [-1, 512, 16, 16]               0\n",
      "         Permute-336          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-337          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-338          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-339          [-1, 512, 16, 16]               0\n",
      "          Conv2d-340          [-1, 512, 16, 16]          25,600\n",
      "          Linear-341         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-342          [-1, 16, 16, 512]               0\n",
      "            GELU-343         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-344          [-1, 16, 16, 512]           1,024\n",
      "          Linear-345          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-346         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-347          [-1, 512, 16, 16]               0\n",
      "            GELU-348         [-1, 16, 16, 2048]               0\n",
      "          Linear-349          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-350          [-1, 512, 16, 16]               0\n",
      "         CNBlock-351          [-1, 512, 16, 16]               0\n",
      "         Permute-352          [-1, 512, 16, 16]               0\n",
      "          Conv2d-353          [-1, 512, 16, 16]          25,600\n",
      "         Permute-354          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-355          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-356          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-357          [-1, 512, 16, 16]               0\n",
      "          Conv2d-358          [-1, 512, 16, 16]          25,600\n",
      "          Linear-359         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-360          [-1, 16, 16, 512]               0\n",
      "            GELU-361         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-362          [-1, 16, 16, 512]           1,024\n",
      "          Linear-363          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-364         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-365          [-1, 512, 16, 16]               0\n",
      "            GELU-366         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-367          [-1, 512, 16, 16]               0\n",
      "         CNBlock-368          [-1, 512, 16, 16]               0\n",
      "          Linear-369          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-370          [-1, 512, 16, 16]          25,600\n",
      "         Permute-371          [-1, 512, 16, 16]               0\n",
      "         Permute-372          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-373          [-1, 16, 16, 512]           1,024\n",
      " StochasticDepth-374          [-1, 512, 16, 16]               0\n",
      "         CNBlock-375          [-1, 512, 16, 16]               0\n",
      "          Conv2d-376          [-1, 512, 16, 16]          25,600\n",
      "          Linear-377         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-378         [-1, 16, 16, 2048]               0\n",
      "         Permute-379          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-380          [-1, 16, 16, 512]           1,024\n",
      "          Linear-381          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-382          [-1, 512, 16, 16]               0\n",
      "          Linear-383         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-384         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-385          [-1, 512, 16, 16]               0\n",
      "         CNBlock-386          [-1, 512, 16, 16]               0\n",
      "          Linear-387          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-388          [-1, 512, 16, 16]          25,600\n",
      "         Permute-389          [-1, 512, 16, 16]               0\n",
      "         Permute-390          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-391          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-392          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-393          [-1, 512, 16, 16]               0\n",
      "          Conv2d-394          [-1, 512, 16, 16]          25,600\n",
      "          Linear-395         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-396          [-1, 16, 16, 512]               0\n",
      "            GELU-397         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-398          [-1, 16, 16, 512]           1,024\n",
      "          Linear-399          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-400         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-401          [-1, 512, 16, 16]               0\n",
      "            GELU-402         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-403          [-1, 512, 16, 16]               0\n",
      "         CNBlock-404          [-1, 512, 16, 16]               0\n",
      "          Linear-405          [-1, 16, 16, 512]       1,049,088\n",
      "          Conv2d-406          [-1, 512, 16, 16]          25,600\n",
      "         Permute-407          [-1, 512, 16, 16]               0\n",
      "         Permute-408          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-409          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-410          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-411          [-1, 512, 16, 16]               0\n",
      "          Conv2d-412          [-1, 512, 16, 16]          25,600\n",
      "          Linear-413         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-414          [-1, 16, 16, 512]               0\n",
      "            GELU-415         [-1, 16, 16, 2048]               0\n",
      "       LayerNorm-416          [-1, 16, 16, 512]           1,024\n",
      "          Linear-417          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-418         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-419          [-1, 512, 16, 16]               0\n",
      "            GELU-420         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-421          [-1, 512, 16, 16]               0\n",
      "          Linear-422          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-423          [-1, 512, 16, 16]               0\n",
      "         Permute-424          [-1, 512, 16, 16]               0\n",
      "          Conv2d-425          [-1, 512, 16, 16]          25,600\n",
      "         Permute-426          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-427          [-1, 512, 16, 16]               0\n",
      "         CNBlock-428          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-429          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-430          [-1, 512, 16, 16]          25,600\n",
      "          Linear-431         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-432          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-433          [-1, 16, 16, 512]           1,024\n",
      "            GELU-434         [-1, 16, 16, 2048]               0\n",
      "          Linear-435          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-436         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-437          [-1, 512, 16, 16]               0\n",
      "            GELU-438         [-1, 16, 16, 2048]               0\n",
      "          Linear-439          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-440          [-1, 512, 16, 16]               0\n",
      "         CNBlock-441          [-1, 512, 16, 16]               0\n",
      "         Permute-442          [-1, 512, 16, 16]               0\n",
      "          Conv2d-443          [-1, 512, 16, 16]          25,600\n",
      "         Permute-444          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-445          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-446          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-447          [-1, 512, 16, 16]               0\n",
      "          Conv2d-448          [-1, 512, 16, 16]          25,600\n",
      "          Linear-449         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-450          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-451          [-1, 16, 16, 512]           1,024\n",
      "            GELU-452         [-1, 16, 16, 2048]               0\n",
      "          Linear-453          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-454         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-455          [-1, 512, 16, 16]               0\n",
      "            GELU-456         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-457          [-1, 512, 16, 16]               0\n",
      "          Linear-458          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-459          [-1, 512, 16, 16]               0\n",
      "         Permute-460          [-1, 512, 16, 16]               0\n",
      "          Conv2d-461          [-1, 512, 16, 16]          25,600\n",
      "         Permute-462          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-463          [-1, 512, 16, 16]               0\n",
      "         CNBlock-464          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-465          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-466          [-1, 512, 16, 16]          25,600\n",
      "         Permute-467          [-1, 16, 16, 512]               0\n",
      "          Linear-468         [-1, 16, 16, 2048]       1,050,624\n",
      "       LayerNorm-469          [-1, 16, 16, 512]           1,024\n",
      "            GELU-470         [-1, 16, 16, 2048]               0\n",
      "          Linear-471         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-472          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-473          [-1, 512, 16, 16]               0\n",
      "            GELU-474         [-1, 16, 16, 2048]               0\n",
      "          Linear-475          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-476          [-1, 512, 16, 16]               0\n",
      "         CNBlock-477          [-1, 512, 16, 16]               0\n",
      "         Permute-478          [-1, 512, 16, 16]               0\n",
      "          Conv2d-479          [-1, 512, 16, 16]          25,600\n",
      "         Permute-480          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-481          [-1, 512, 16, 16]               0\n",
      "         CNBlock-482          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-483          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-484          [-1, 512, 16, 16]          25,600\n",
      "          Linear-485         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-486          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-487          [-1, 16, 16, 512]           1,024\n",
      "            GELU-488         [-1, 16, 16, 2048]               0\n",
      "          Linear-489          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-490         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-491          [-1, 512, 16, 16]               0\n",
      "            GELU-492         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-493          [-1, 512, 16, 16]               0\n",
      "          Linear-494          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-495          [-1, 512, 16, 16]               0\n",
      "         Permute-496          [-1, 512, 16, 16]               0\n",
      "          Conv2d-497          [-1, 512, 16, 16]          25,600\n",
      "         Permute-498          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-499          [-1, 512, 16, 16]               0\n",
      "         CNBlock-500          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-501          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-502          [-1, 512, 16, 16]          25,600\n",
      "         Permute-503          [-1, 16, 16, 512]               0\n",
      "          Linear-504         [-1, 16, 16, 2048]       1,050,624\n",
      "       LayerNorm-505          [-1, 16, 16, 512]           1,024\n",
      "            GELU-506         [-1, 16, 16, 2048]               0\n",
      "          Linear-507         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-508          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-509          [-1, 512, 16, 16]               0\n",
      "            GELU-510         [-1, 16, 16, 2048]               0\n",
      "          Linear-511          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-512          [-1, 512, 16, 16]               0\n",
      "         CNBlock-513          [-1, 512, 16, 16]               0\n",
      "         Permute-514          [-1, 512, 16, 16]               0\n",
      "          Conv2d-515          [-1, 512, 16, 16]          25,600\n",
      "         Permute-516          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-517          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-518          [-1, 16, 16, 512]           1,024\n",
      "         CNBlock-519          [-1, 512, 16, 16]               0\n",
      "          Conv2d-520          [-1, 512, 16, 16]          25,600\n",
      "          Linear-521         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-522          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-523          [-1, 16, 16, 512]           1,024\n",
      "            GELU-524         [-1, 16, 16, 2048]               0\n",
      "          Linear-525          [-1, 16, 16, 512]       1,049,088\n",
      "          Linear-526         [-1, 16, 16, 2048]       1,050,624\n",
      "         Permute-527          [-1, 512, 16, 16]               0\n",
      "            GELU-528         [-1, 16, 16, 2048]               0\n",
      " StochasticDepth-529          [-1, 512, 16, 16]               0\n",
      "          Linear-530          [-1, 16, 16, 512]       1,049,088\n",
      "         CNBlock-531          [-1, 512, 16, 16]               0\n",
      "         Permute-532          [-1, 512, 16, 16]               0\n",
      "          Conv2d-533          [-1, 512, 16, 16]          25,600\n",
      "         Permute-534          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-535          [-1, 512, 16, 16]               0\n",
      "         CNBlock-536          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-537          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-538          [-1, 512, 16, 16]          25,600\n",
      "         Permute-539          [-1, 16, 16, 512]               0\n",
      "          Linear-540         [-1, 16, 16, 2048]       1,050,624\n",
      "       LayerNorm-541          [-1, 16, 16, 512]           1,024\n",
      "            GELU-542         [-1, 16, 16, 2048]               0\n",
      "          Linear-543         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-544          [-1, 16, 16, 512]       1,049,088\n",
      "            GELU-545         [-1, 16, 16, 2048]               0\n",
      "         Permute-546          [-1, 512, 16, 16]               0\n",
      "          Linear-547          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-548          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-549          [-1, 512, 16, 16]               0\n",
      "         CNBlock-550          [-1, 512, 16, 16]               0\n",
      "          Conv2d-551          [-1, 512, 16, 16]          25,600\n",
      " StochasticDepth-552          [-1, 512, 16, 16]               0\n",
      "         CNBlock-553          [-1, 512, 16, 16]               0\n",
      "         Permute-554          [-1, 16, 16, 512]               0\n",
      "          Conv2d-555          [-1, 512, 16, 16]          25,600\n",
      "       LayerNorm-556          [-1, 16, 16, 512]           1,024\n",
      "         Permute-557          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-558          [-1, 16, 16, 512]           1,024\n",
      "          Linear-559         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-560         [-1, 16, 16, 2048]               0\n",
      "          Linear-561         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-562          [-1, 16, 16, 512]       1,049,088\n",
      "            GELU-563         [-1, 16, 16, 2048]               0\n",
      "         Permute-564          [-1, 512, 16, 16]               0\n",
      "          Linear-565          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-566          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-567          [-1, 512, 16, 16]               0\n",
      "         CNBlock-568          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-569          [-1, 512, 16, 16]               0\n",
      "          Conv2d-570          [-1, 512, 16, 16]          25,600\n",
      "         CNBlock-571          [-1, 512, 16, 16]               0\n",
      "         Permute-572          [-1, 16, 16, 512]               0\n",
      "          Conv2d-573          [-1, 512, 16, 16]          25,600\n",
      "       LayerNorm-574          [-1, 16, 16, 512]           1,024\n",
      "         Permute-575          [-1, 16, 16, 512]               0\n",
      "       LayerNorm-576          [-1, 16, 16, 512]           1,024\n",
      "          Linear-577         [-1, 16, 16, 2048]       1,050,624\n",
      "            GELU-578         [-1, 16, 16, 2048]               0\n",
      "          Linear-579         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-580          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-581          [-1, 512, 16, 16]               0\n",
      "            GELU-582         [-1, 16, 16, 2048]               0\n",
      "          Linear-583          [-1, 16, 16, 512]       1,049,088\n",
      " StochasticDepth-584          [-1, 512, 16, 16]               0\n",
      "         CNBlock-585          [-1, 512, 16, 16]               0\n",
      "         Permute-586          [-1, 512, 16, 16]               0\n",
      "          Conv2d-587          [-1, 512, 16, 16]          25,600\n",
      "         Permute-588          [-1, 16, 16, 512]               0\n",
      " StochasticDepth-589          [-1, 512, 16, 16]               0\n",
      "         CNBlock-590          [-1, 512, 16, 16]               0\n",
      "       LayerNorm-591          [-1, 16, 16, 512]           1,024\n",
      "          Conv2d-592          [-1, 512, 16, 16]          25,600\n",
      "         Permute-593          [-1, 16, 16, 512]               0\n",
      "          Linear-594         [-1, 16, 16, 2048]       1,050,624\n",
      "       LayerNorm-595          [-1, 16, 16, 512]           1,024\n",
      "            GELU-596         [-1, 16, 16, 2048]               0\n",
      "          Linear-597         [-1, 16, 16, 2048]       1,050,624\n",
      "          Linear-598          [-1, 16, 16, 512]       1,049,088\n",
      "            GELU-599         [-1, 16, 16, 2048]               0\n",
      "         Permute-600          [-1, 512, 16, 16]               0\n",
      "          Linear-601          [-1, 16, 16, 512]       1,049,088\n",
      "         Permute-602          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-603          [-1, 512, 16, 16]               0\n",
      "         CNBlock-604          [-1, 512, 16, 16]               0\n",
      " StochasticDepth-605          [-1, 512, 16, 16]               0\n",
      "     LayerNorm2d-606          [-1, 512, 16, 16]           1,024\n",
      "         CNBlock-607          [-1, 512, 16, 16]               0\n",
      "     LayerNorm2d-608          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-609           [-1, 1024, 8, 8]       2,098,176\n",
      "          Conv2d-610           [-1, 1024, 8, 8]       2,098,176\n",
      "          Conv2d-611           [-1, 1024, 8, 8]          51,200\n",
      "          Conv2d-612           [-1, 1024, 8, 8]          51,200\n",
      "         Permute-613           [-1, 8, 8, 1024]               0\n",
      "       LayerNorm-614           [-1, 8, 8, 1024]           2,048\n",
      "         Permute-615           [-1, 8, 8, 1024]               0\n",
      "       LayerNorm-616           [-1, 8, 8, 1024]           2,048\n",
      "          Linear-617           [-1, 8, 8, 4096]       4,198,400\n",
      "          Linear-618           [-1, 8, 8, 4096]       4,198,400\n",
      "            GELU-619           [-1, 8, 8, 4096]               0\n",
      "            GELU-620           [-1, 8, 8, 4096]               0\n",
      "          Linear-621           [-1, 8, 8, 1024]       4,195,328\n",
      "         Permute-622           [-1, 1024, 8, 8]               0\n",
      "          Linear-623           [-1, 8, 8, 1024]       4,195,328\n",
      " StochasticDepth-624           [-1, 1024, 8, 8]               0\n",
      "         Permute-625           [-1, 1024, 8, 8]               0\n",
      "         CNBlock-626           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-627           [-1, 1024, 8, 8]          51,200\n",
      " StochasticDepth-628           [-1, 1024, 8, 8]               0\n",
      "         CNBlock-629           [-1, 1024, 8, 8]               0\n",
      "         Permute-630           [-1, 8, 8, 1024]               0\n",
      "       LayerNorm-631           [-1, 8, 8, 1024]           2,048\n",
      "          Conv2d-632           [-1, 1024, 8, 8]          51,200\n",
      "         Permute-633           [-1, 8, 8, 1024]               0\n",
      "       LayerNorm-634           [-1, 8, 8, 1024]           2,048\n",
      "          Linear-635           [-1, 8, 8, 4096]       4,198,400\n",
      "            GELU-636           [-1, 8, 8, 4096]               0\n",
      "          Linear-637           [-1, 8, 8, 4096]       4,198,400\n",
      "          Linear-638           [-1, 8, 8, 1024]       4,195,328\n",
      "            GELU-639           [-1, 8, 8, 4096]               0\n",
      "         Permute-640           [-1, 1024, 8, 8]               0\n",
      "          Linear-641           [-1, 8, 8, 1024]       4,195,328\n",
      "         Permute-642           [-1, 1024, 8, 8]               0\n",
      " StochasticDepth-643           [-1, 1024, 8, 8]               0\n",
      "         CNBlock-644           [-1, 1024, 8, 8]               0\n",
      " StochasticDepth-645           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-646           [-1, 1024, 8, 8]          51,200\n",
      "         CNBlock-647           [-1, 1024, 8, 8]               0\n",
      "         Permute-648           [-1, 8, 8, 1024]               0\n",
      "          Conv2d-649           [-1, 1024, 8, 8]          51,200\n",
      "       LayerNorm-650           [-1, 8, 8, 1024]           2,048\n",
      "         Permute-651           [-1, 8, 8, 1024]               0\n",
      "       LayerNorm-652           [-1, 8, 8, 1024]           2,048\n",
      "          Linear-653           [-1, 8, 8, 4096]       4,198,400\n",
      "            GELU-654           [-1, 8, 8, 4096]               0\n",
      "          Linear-655           [-1, 8, 8, 4096]       4,198,400\n",
      "          Linear-656           [-1, 8, 8, 1024]       4,195,328\n",
      "            GELU-657           [-1, 8, 8, 4096]               0\n",
      "         Permute-658           [-1, 1024, 8, 8]               0\n",
      "          Linear-659           [-1, 8, 8, 1024]       4,195,328\n",
      "         Permute-660           [-1, 1024, 8, 8]               0\n",
      " StochasticDepth-661           [-1, 1024, 8, 8]               0\n",
      "         CNBlock-662           [-1, 1024, 8, 8]               0\n",
      " StochasticDepth-663           [-1, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-664           [-1, 1024, 1, 1]               0\n",
      "         CNBlock-665           [-1, 1024, 8, 8]               0\n",
      "     LayerNorm2d-666           [-1, 1024, 1, 1]           2,048\n",
      "AdaptiveAvgPool2d-667           [-1, 1024, 1, 1]               0\n",
      "         Flatten-668                 [-1, 1024]               0\n",
      "     LayerNorm2d-669           [-1, 1024, 1, 1]           2,048\n",
      "         Flatten-670                 [-1, 1024]               0\n",
      "          Linear-671                    [-1, 1]           1,025\n",
      "        ConvNeXt-672                    [-1, 1]               0\n",
      "          Linear-673                    [-1, 1]           1,025\n",
      "        ConvNeXt-674                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 175,098,882\n",
      "Trainable params: 175,098,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 1432.05\n",
      "Params size (MB): 667.95\n",
      "Estimated Total Size (MB): 2100.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f79b7301-e35a-43cd-ad62-50b514ba1202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BCEWithLogitsLoss(),\n",
       " Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "# dice = Dice(average='micro').to(device)\n",
    "loss_fn , optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d765f61-bc75-4f16-aab6-6e1d0625d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafd4fa-97a6-439e-991d-132610f462c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "train(epochs, model, train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7818521-e007-4fa0-bc14-6c2fb2c66829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy \n",
    "acc = Accuracy(task=\"binary\", num_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30622bb7-271d-4b19-b339-22d96306ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_loss = 0\n",
    "    test_dice_coefficient = []\n",
    "    test_dice_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            image, label = data\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # calculate dice coefficient and dice loss\n",
    "            #pred = (output > 0.5).float()\n",
    "            # label = label.to(torch.int64)\n",
    "            testdice = acc(output, label)\n",
    "            # dice_loss = 1 - testdice\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_dice_coefficient.append(testdice.item())\n",
    "            # test_dice_loss += dice_loss.item()\n",
    "\n",
    "        # calculate average test loss, dice coefficient, and dice loss\n",
    "        #print(len(test_dice_coefficient))\n",
    "    test_loss /= len(test_loader)\n",
    "    avg_test_dice_coefficient = sum(test_dice_coefficient)/len(test_dice_coefficient)\n",
    "    # test_dice_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.5f} | Accuracy: {avg_test_dice_coefficient:.5f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8ea9f90-c420-4117-88cc-a0ea2bdbe4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "def get_roc_auc(model, test_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            image, label = data\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            output = model(image)\n",
    "            output = torch.sigmoid(output)  # Apply sigmoid activation for probability\n",
    "            output = output.detach().cpu().numpy().flatten()\n",
    "            label = label.detach().cpu().numpy().flatten()\n",
    "\n",
    "            y_scores.extend(output)\n",
    "            y_true.extend(label)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(y_true, y_scores)\n",
    "#     # Plot ROC curve\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "#     plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver Operating Characteristic')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()\n",
    "\n",
    "#     # Compute accuracy\n",
    "#     # y_pred = np.round(y_scores)  # Round probabilities to binary predictions (0 or 1)\n",
    "#     accuracy = accuracy_score(y_true, y_scores)\n",
    "#     print('Accuracy: {:.5f}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "169aef5c-0341-48be-b1e2-6fefcca96f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] [0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192423, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192422, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419, 0.20192419]\n"
     ]
    }
   ],
   "source": [
    "get_roc_auc(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bc95e8f-53ce-4070-8c7d-09392efdc4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 1., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(test_loader))\n",
    "imgs, lbls = sample\n",
    "\n",
    "actual_imagelabels = lbls[:10].numpy()\n",
    "actual_imagelabels.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc74f3-bfdf-4d1e-a873-1870dcf8490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "528bf615-f51c-4ce2-978e-81321b0bf8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction labels: [0 0 0 0 0 0 0 0 0 0]\n",
      "Actual labels: [0. 1. 0. 0. 0. 0. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_output= model(imgs[:10])\n",
    "x = torch.sigmoid(test_output)>0.5 \n",
    "x = x.int()\n",
    "\n",
    "print(f'Prediction labels: {x.cpu().numpy().squeeze()}')\n",
    "print(f'Actual labels: {actual_imagelabels.squeeze()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f845073-0690-47ee-b72c-2965b530bccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((1024,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.module.features[7][1].norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd60c855-0513-4cbf-abf4-4c59de09a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, deprocess_image, preprocess_image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "import os\n",
    "path = \"/scratch/akaniyar/colonoscopy/images/\"\n",
    "x=[]\n",
    "files = os.listdir(path)[:10]\n",
    "for f in files :\n",
    "    if os.path.isfile(path+'/'+f):\n",
    "        x.append(f)\n",
    "\n",
    "# print(ClassifierOutputTarget(0))\n",
    "targets = [ClassifierOutputTarget(0)]\n",
    "target_layers = [model.module.features]\n",
    "y=0\n",
    "for i in x:\n",
    "    img = cv2.imread(path+i)\n",
    "    #print(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.float32(img)/np.max(img)\n",
    "    #print(img)\n",
    "    input_tensor = preprocess_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    #print(input_tensor.shape)\n",
    "    with GradCAM(model=model, target_layers=target_layers) as cam:\n",
    "        grayscale_cams = cam(input_tensor=input_tensor, targets=targets)\n",
    "        cam_image = show_cam_on_image(img, grayscale_cams[0, :], use_rgb=True)\n",
    "        cam = np.uint8(255*grayscale_cams[0, :])\n",
    "        cam = cv2.merge([cam, cam, cam])\n",
    "        images = np.hstack((np.uint8(255*img),cam_image))\n",
    "        x = Image.fromarray(images)\n",
    "        x.save(f\"/home/akaniyar/Colonoscopy/grad_cam/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f724c1-bb1f-4702-aae6-5a7ca1cafef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622366c-d6d6-42e5-8d8c-c035bf547f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
